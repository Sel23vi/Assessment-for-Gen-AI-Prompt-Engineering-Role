{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4419cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Hugging Face Transformers\n",
    "# !pip install transformers\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "def generate_text(prompt, model_name='gpt2', max_length=50, temperature=0.7):\n",
    "    # Load pre-trained model and tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Encode the input prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate text\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_length=max_length,  # Maximum length of the generated text\n",
    "        do_sample=True,         # Sampling to generate more creative text\n",
    "        temperature=temperature, # Controls the creativity (higher temperature means more randomness)\n",
    "        top_k=50,                # Limits the sampling pool to the top 50 words for less randomness\n",
    "        top_p=0.95,              # Nucleus sampling to focus on likely words\n",
    "        num_return_sequences=1   # Number of sequences to return\n",
    "    )\n",
    "    \n",
    "    # Decode and return the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Once upon a time\"\n",
    "generated_text = generate_text(prompt, max_length=100, temperature=0.8)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b065e42",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "Model and Tokenizer:\n",
    "GPT-2 is loaded using Hugging Face's GPT2LMHeadModel and GPT2Tokenizer. You can use other pre-trained models from Hugging Face's model hub by changing the model_name.\n",
    "\n",
    "\n",
    "Text Generation:\n",
    "The generate method from the model generates text based on the input prompt. Parameters like max_length, temperature, top_k, and top_p control the creativity and randomness of the text generation.\n",
    "\n",
    "\n",
    "Temperature:\n",
    "A lower temperature produces more predictable text, while a higher temperature allows for more creative and diverse outputs.\n",
    "\n",
    "\n",
    "Top-k Sampling:\n",
    "Limits the generation to the top 50 most probable next words.\n",
    "\n",
    "\n",
    "Top-p (Nucleus) Sampling:\n",
    "Focuses on a dynamic number of probable words by capping the cumulative probability to 0.95."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8924e1b6",
   "metadata": {},
   "source": [
    "Example Output:\n",
    "For the input prompt: \"Once upon a time\"\n",
    "\n",
    "The generated output could be something like: \"Once upon a time, there was a young boy who lived in a small village at the edge of a great forest. He dreamed of becoming a hero and fighting against the dragons that had terrorized the land for centuries...\"\n",
    "\n",
    "The approach gives flexibility and control over the quality and length of the generated text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
